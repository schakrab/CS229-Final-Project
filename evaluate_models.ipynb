{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a mapping between different data formats. Not clean, but works!\n",
    "\n",
    "name_cities_mapping = {\n",
    "\t\"Anaheim\": \"ANA\", \n",
    "\t\"Anaheim Ducks\":\"ANA\", \n",
    "\n",
    "\t\"Arizona\": \"ARI\", \n",
    "\t\"Phoenix\": \"ARI\", \n",
    "\t\"Phoenix Coyotes\":\"ARI\", \n",
    "\n",
    "\t\"Boston\": \"BOS\", \n",
    "\t\"Boston Bruins\":\"BOS\", \n",
    "\n",
    "\t\"Buffalo\": \"BUF\", \n",
    "\t\"Buffalo Sabres\":\"BUF\", \n",
    "\n",
    "\t\"Carolina\": \"CAR\", \n",
    "\t\"Carolina Hurricanes\":\"CAR\", \n",
    "\n",
    "\t\"Columbus\": \"CBJ\", \n",
    "\t\"Columbus Blue Jackets\":\"CBJ\", \n",
    "\n",
    "\t\"Calgary\": \"CGY\", \n",
    "\t\"Calgary Flames\":\"CGY\", \n",
    "\n",
    "\t\"Chicago\": \"CHI\", \n",
    "\t\"Chicago Blackhawks\":\"CHI\", \n",
    "\n",
    "\t\"Colorado\": \"COL\", \n",
    "\t\"Colorado Avalanche\":\"COL\", \n",
    "\n",
    "\t\"Dallas\": \"DAL\", \n",
    "\t\"Dallas Stars\":\"DAL\", \n",
    "\n",
    "\t\"Detroit\": \"DET\", \n",
    "\t\"Detroit Red Wings\":\"DET\", \n",
    "\n",
    "\t\"Edmonton\":\"EDM\", \n",
    "\t\"Edmonton Oilers\":\"EDM\", \n",
    "\n",
    "\t\"Florida\":\"FLA\", \n",
    "\t\"Florida Panthers\":\"FLA\", \n",
    "\n",
    "\t\"LosAngeles\":\"L.A\",\n",
    "\t\"Los Angeles\":\"L.A\", \n",
    "\t\"Los Angeles Kings\":\"L.A\", \n",
    "\n",
    "\t\"Minnesota\":\"MIN\", \n",
    "\t\"Minnesota Wild\":\"MIN\", \n",
    "\n",
    "\t\"Montreal\":\"MTL\", \n",
    "\t\"Montreal Canadiens\":\"MTL\", \n",
    "\n",
    "\t\"NewJersey\":\"N.J\", \n",
    "\t\"New Jersey\":\"N.J\", \n",
    "\t\"New Jersey Devils\":\"N.J\", \n",
    "\n",
    "\t\"Nashville\":\"NSH\", \n",
    "\t\"Nashville Predators\":\"NSH\", \n",
    "\n",
    "\t\"NYIslanders\":\"NYI\", \n",
    "\t\"NY Islanders\":\"NYI\", \n",
    "\t\"New York Islanders\":\"NYI\",\n",
    "\n",
    "\t\"NYRangers\":\"NYR\",\n",
    "\t\"NY Rangers\":\"NYR\", \n",
    "\t\"New York Rangers\":\"NYR\", \n",
    "\n",
    "\t\"Ottawa\":\"OTT\", \n",
    "\t\"Ottawa Senators\":\"OTT\", \n",
    "\n",
    "\t\"Philadelphia\":\"PHI\", \n",
    "\t\"Philadelphia Flyers\":\"PHI\", \n",
    "\n",
    "\t\"Pittsburgh\":\"PIT\", \n",
    "\t\"Pittsburgh Penguins\":\"PIT\", \n",
    "\n",
    "\t\"SanJose\":\"S.J\",\n",
    "\t\"San Jose\":\"S.J\", \n",
    "\t\"San Jose Sharks\":\"S.J\", \n",
    "\n",
    "\t\"St.Louis\":\"STL\", \n",
    "\t\"St. Louis\":\"STL\", \n",
    "\t\"St. Louis Blues\":\"STL\", \n",
    "\n",
    "\t\"TampaBay\":\"T.B\",\n",
    "\t\"Tampa Bay\":\"T.B\", \n",
    "\t\"Tampa Bay Lightning\":\"T.B\", \n",
    "\n",
    "\t\"Toronto\":\"TOR\", \n",
    "\t\"Toronto Maple Leafs\":\"TOR\", \n",
    "\n",
    "\t\"Vancouver\":\"VAN\", \n",
    "\t\"Vancouver Canucks\":\"VAN\", \n",
    "\n",
    "\t\"Winnipeg\":\"WPG\", \n",
    "\t\"Winnipeg Jets\":\"WPG\", \n",
    "\n",
    "\t\"Washington\":\"WSH\",\n",
    "\t\"Washington Capitals\":\"WSH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the features from the csv file. \n",
    "# Simple features found in team_stats.csv\n",
    "# Advanced features found in team_stats_2017-12-03.csv and team_stats_2017-12-04.csv\n",
    "# Difference between 03 and 04 (ignore the date) is that 03 is 5on5 games, and 04 is all games.\n",
    "features = np.genfromtxt('team_stats_2017-12-03.csv', delimiter=',', skip_header=1, dtype=(\"|S10\", \"|S10\", float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float, float ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility to convert dates between formats used in the various data files.\n",
    "def convert_date(date):\n",
    "    # date is format YYYY-MM-DD, return int\n",
    "    return int(date[0:4])*10000 + int(date[5:7])*100 + int(date[8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the date that lies in the particular season.\n",
    "def find_season(seasons, date):\n",
    "    for i in range(len(seasons)):\n",
    "        if date < seasons[i]:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the name of each feature from the file (first line).\n",
    "feature_names = np.genfromtxt('team_stats_2017-12-03.csv', dtype='str', delimiter=',', skip_header=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \"CF\"\n",
      "5 \"CA\"\n",
      "6 \"C+/-\"\n",
      "8 \"CF/60\"\n",
      "9 \"CA/60\"\n",
      "10 \"GF\"\n",
      "11 \"GA\"\n",
      "12 \"G+/-\"\n",
      "14 \"GF/60\"\n",
      "15 \"GA/60\"\n",
      "16 \"xGF\"\n",
      "17 \"xGA\"\n",
      "18 \"xG+/-\"\n",
      "20 \"xGF/60\"\n",
      "21 \"xGA/60\"\n",
      "22 \"PENT\"\n",
      "23 \"PEND\"\n",
      "24 \"P+/-\"\n",
      "27 \"PDO\"\n",
      "28 \"FF\"\n",
      "29 \"FA\"\n",
      "30 \"F+/-\"\n",
      "32 \"FF/60\"\n",
      "33 \"FA/60\"\n",
      "34 \"SF\"\n",
      "35 \"SA\"\n",
      "36 \"S+/-\"\n",
      "38 \"SF/60\"\n",
      "39 \"SA/60\"\n",
      "40 \"PENT/60\"\n",
      "41 \"PEND/60\"\n",
      "48 \"xPDO\"\n",
      "49 \"dPDO\"\n",
      "50 \"OZS\"\n",
      "51 \"DZS\"\n",
      "52 \"NZS\"\n",
      "56 \"ZSR\"\n",
      "58 \"FOW\"\n",
      "59 \"FOL\"\n",
      "60 \"GVA\"\n",
      "61 \"TKA\"\n",
      "62 \"GVA/60\"\n",
      "63 \"TKA/60\"\n",
      "64 \"HF\"\n",
      "65 \"HA\"\n",
      "66 \"H+/-\"\n",
      "67 \"HF/60\"\n",
      "68 \"HA/60\"\n"
     ]
    }
   ],
   "source": [
    "# Advanced Feature List (Overall Play)\n",
    "adv_keep = []\n",
    "\n",
    "for i in range(feature_names.shape[0]):\n",
    "    if '%' not in feature_names[i] and i > 3:\n",
    "        print(i, feature_names[i])\n",
    "        adv_keep.append(i)\n",
    "\n",
    "# Simple Feature List (Just Shots)\n",
    "simple_keep = [4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 28, 29, 30, 32, 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into a \"mapping\" array (part 1 of creating the feature set)\n",
    "\n",
    "season_names = [\"12-13\", \"13-14\", \"14-15\", \"15-16\", \"16-17\"]\n",
    "seasons = [20130501, 20140501, 20150501, 20160501, 20170501]\n",
    "\n",
    "# games = {} # Season -> Team -> Date -> just date\n",
    "mapping_adv = {} # Season -> Team -> Date -> Average upto date\n",
    "mapping_simp = {}\n",
    "for name in season_names:\n",
    "    mapping_adv[name] = {}\n",
    "    mapping_simp[name] = {}\n",
    "#     games[name] = {}\n",
    "\n",
    "for i in range(features.shape[0]):\n",
    "    name = str(features[i][0], 'utf-8').replace('\"', '').strip()\n",
    "    date = convert_date(str(features[i][1], 'utf-8').strip())\n",
    "    adv_stats = np.asarray(list((features[i])))[adv_keep]\n",
    "    adv_stats = adv_stats.astype(float)\n",
    "    simple_stats = np.asarray(list((features[i])))[simple_keep]\n",
    "    simple_stats = simple_stats.astype(float)\n",
    "    season = find_season(seasons, date)\n",
    "        \n",
    "    # add to mapping[season_names[season]]\n",
    "    if name not in mapping_adv[season_names[season]]:\n",
    "        mapping_adv[season_names[season]][name] = {}\n",
    "#         games[season_names[season]][name] = {}\n",
    "        mapping_adv[season_names[season]][name]['total'] = np.zeros(len(adv_keep))\n",
    "        mapping_adv[season_names[season]][name]['count'] = 0\n",
    "    if name not in mapping_simp[season_names[season]]:\n",
    "        mapping_simp[season_names[season]][name] = {}\n",
    "#         games[season_names[season]][name] = {}\n",
    "        mapping_simp[season_names[season]][name]['total'] = np.zeros(len(simple_keep))\n",
    "        mapping_simp[season_names[season]][name]['count'] = 0\n",
    "\n",
    "        \n",
    "#     games[season_names[season]][name][date] = stats\n",
    "\n",
    "    if mapping_adv[season_names[season]][name]['count'] != 0:\n",
    "        mapping_adv[season_names[season]][name][date] = [mapping_adv[season_names[season]][name]['count'], mapping_adv[season_names[season]][name]['total'] / mapping_adv[season_names[season]][name]['count']]\n",
    "\n",
    "    if mapping_simp[season_names[season]][name]['count'] != 0:\n",
    "        mapping_simp[season_names[season]][name][date] = [mapping_simp[season_names[season]][name]['count'], mapping_simp[season_names[season]][name]['total'] / mapping_simp[season_names[season]][name]['count']]\n",
    "        \n",
    "\n",
    "    mapping_adv[season_names[season]][name]['count'] += 1        \n",
    "    mapping_adv[season_names[season]][name]['total'] += adv_stats\n",
    "        \n",
    "    mapping_simp[season_names[season]][name]['count'] += 1        \n",
    "    mapping_simp[season_names[season]][name]['total'] += simple_stats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the format of dates used in the odds file.\n",
    "\n",
    "def convert_odds_date(entry, year):\n",
    "\tif len(entry) == 4:\n",
    "\t\treturn int(year)*10000 + int(entry[0:2])*100 + int(entry[2:])\n",
    "\telse:\n",
    "\t\treturn int(year+1)*10000 + int(entry[0])*100 + int(entry[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Was used when trying out different values of hyperparameter \"K\"\n",
    "# which relates to the last number of games that are averaged (instead of\n",
    "# looking at the entire season). Varying K doesn't impact performance, so\n",
    "# this function is no longer used.\n",
    "\n",
    "# DEPRECATED\n",
    "def get_last_N_games_average(games, season, team, current, N, lower_bound):\n",
    "    print(\"Warning: get_last_N_games_average is deprecated\")\n",
    "    return None # keep (deprecated)\n",
    "    avg_games = np.zeros(len(keep))\n",
    "    count = 0\n",
    "    \n",
    "    while count < N:\n",
    "        current -= 1\n",
    "        if current in games[season][team]:\n",
    "            avg_games += games[season][team][current]\n",
    "            count += 1\n",
    "        if current < lower_bound:\n",
    "            break\n",
    "    \n",
    "    if current < lower_bound:\n",
    "        return None\n",
    "    \n",
    "    avg_games /= count\n",
    "    return avg_games\n",
    "\n",
    "lower_bounds = [20120501, 20130501, 20140501, 20150501, 20160501]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading the odds file, keep track of wins/losses for teams up to that point\n",
    "\n",
    "def load_wins_losses(txtfile, year, mapping, season):\n",
    "    with open(txtfile, 'r') as f:\n",
    "        lines = [row for row in csv.reader(f.read().splitlines())]\n",
    "        lines.pop(0)\n",
    "    \n",
    "        for i in range(0, len(lines), 2):\n",
    "            line1 = lines[i]\n",
    "            line2 = lines[i+1]\n",
    "            \n",
    "            if line1[7].strip() == '' or line2[7].strip() == '':\n",
    "                continue\n",
    "            \n",
    "            date = convert_odds_date(line1[0], year)\n",
    "            team1 = name_cities_mapping[line1[3].strip()] # Visitor\n",
    "            team2 = name_cities_mapping[line2[3].strip()] # Home\n",
    "            score1 = int(line1[7])\n",
    "            score2 = int(line2[7])\n",
    "\n",
    "            if date not in mapping[season][team2] or date not in mapping[season][team1]:\n",
    "                continue\n",
    "            \n",
    "            if 'wins' not in mapping[season][team1]:\n",
    "                mapping[season][team1]['wins'] = 0\n",
    "                mapping[season][team1]['losses'] = 0\n",
    "                mapping[season][team1][date][1] = np.append(mapping[season][team1][date][1], 0)\n",
    "            else:\n",
    "                pct1 = mapping[season][team1]['wins'] * 1. / (mapping[season][team1]['wins'] + mapping[season][team1]['losses'])\n",
    "                mapping[season][team1][date][1] = np.append(mapping[season][team1][date][1], pct1)\n",
    "                \n",
    "            if 'wins' not in mapping[season][team2]:\n",
    "                mapping[season][team2]['wins'] = 0\n",
    "                mapping[season][team2]['losses'] = 0\n",
    "                mapping[season][team2][date][1] = np.append(mapping[season][team2][date][1], 0)\n",
    "            else:\n",
    "                pct2 = mapping[season][team2]['wins'] * 1. / (mapping[season][team2]['wins'] + mapping[season][team2]['losses'])\n",
    "                mapping[season][team2][date][1] = np.append(mapping[season][team2][date][1], pct2)\n",
    "\n",
    "            if score2 > score1:\n",
    "                mapping[season][team2]['wins'] += 1\n",
    "                mapping[season][team1]['losses'] += 1\n",
    "            else:\n",
    "                mapping[season][team1]['wins'] += 1\n",
    "                mapping[season][team2]['losses'] += 1\n",
    "    \n",
    "load_wins_losses(\"nhl_odds_2012-13.csv\", 2012, mapping_adv, season_names[0])\n",
    "load_wins_losses(\"nhl_odds_2013-14.csv\", 2013, mapping_adv, season_names[1])\n",
    "load_wins_losses(\"nhl_odds_2014-15.csv\", 2014, mapping_adv, season_names[2])\n",
    "load_wins_losses(\"nhl_odds_2015-16.csv\", 2015, mapping_adv, season_names[3])\n",
    "load_wins_losses(\"nhl_odds_2016-17.csv\", 2016, mapping_adv, season_names[4])\n",
    "\n",
    "load_wins_losses(\"nhl_odds_2012-13.csv\", 2012, mapping_simp, season_names[0])\n",
    "load_wins_losses(\"nhl_odds_2013-14.csv\", 2013, mapping_simp, season_names[1])\n",
    "load_wins_losses(\"nhl_odds_2014-15.csv\", 2014, mapping_simp, season_names[2])\n",
    "load_wins_losses(\"nhl_odds_2015-16.csv\", 2015, mapping_simp, season_names[3])\n",
    "load_wins_losses(\"nhl_odds_2016-17.csv\", 2016, mapping_simp, season_names[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct the feature set. Initially, we tried out various types of feature sets (code is still here, just commented).\n",
    "# Those included looking at the last 5 games, the last 3, the last 10, the last 20, the last 40,\n",
    "# just those games, or a combination of those with the overall season.\n",
    "# We also tried representing the features as a difference between two teams or just in vector form.\n",
    "\n",
    "# Vector form gave the model more flexibility/was better on the dev set, so we stuck with that.\n",
    "\n",
    "# simple_features_diff = []\n",
    "simple_features = []\n",
    "advanced_features = []\n",
    "\n",
    "#features_justlast5_diff = []\n",
    "#features_justlast5_vec = []\n",
    "\n",
    "#features_alsolast5_diff = []\n",
    "#features_alsolast5_vec = []\n",
    "#features_alsolast3_vec = []\n",
    "# features_alsolast10_vec = []\n",
    "# features_alsolast20_vec = []\n",
    "# features_alsolast30_vec = []\n",
    "#features_alsolast40_vec = []\n",
    "\n",
    "labels_single_simp = []\n",
    "labels_multi_simp = []\n",
    "N_simp = []\n",
    "\n",
    "labels_single_adv = []\n",
    "labels_multi_adv = []\n",
    "N_adv = []\n",
    "\n",
    "def load_odds_file(txtfile, year, mapping, season, lower_bound, labelss, labelsm, Narr, featuresarr):\n",
    "    with open(txtfile, 'r') as f:\n",
    "        lines = [row for row in csv.reader(f.read().splitlines())]\n",
    "        lines.pop(0)\n",
    "        \n",
    "        for i in range(0, len(lines), 2):\n",
    "            line1 = lines[i]\n",
    "            line2 = lines[i+1]\n",
    "            \n",
    "            if line1[7].strip() == '' or line2[7].strip() == '':\n",
    "                continue\n",
    "            \n",
    "            #print(line1, line2)\n",
    "            \n",
    "            date = convert_odds_date(line1[0], year)\n",
    "            team1 = name_cities_mapping[line1[3].strip()] # Visitor\n",
    "            team2 = name_cities_mapping[line2[3].strip()] # Home\n",
    "            score1 = int(line1[7])\n",
    "            score2 = int(line2[7])\n",
    "            \n",
    "            #print team1, team2, date, score1, score2\n",
    "            \n",
    "            if date not in mapping[season][team2] or date not in mapping[season][team1]:\n",
    "                continue\n",
    "            \n",
    "#             features_diff.append(mapping[season][team2][date][1] - mapping[season][team1][date][1])\n",
    "            \n",
    "            vec = []\n",
    "            vec.extend(mapping[season][team2][date][1])\n",
    "            vec.extend(mapping[season][team1][date][1])\n",
    "            featuresarr.append(vec)\n",
    "\n",
    "# Old (not clean) code for the old feature sets commented out:\n",
    "#             features_last5_team1 = get_last_N_games_average(games, season, team1, date, 5, lower_bound)\n",
    "#             features_last5_team2 = get_last_N_games_average(games, season, team2, date, 5, lower_bound)\n",
    "\n",
    "#             features_last3_team1 = get_last_N_games_average(games, season, team1, date, 3, lower_bound)\n",
    "#             features_last3_team2 = get_last_N_games_average(games, season, team2, date, 3, lower_bound)\n",
    "\n",
    "#             features_last10_team1 = get_last_N_games_average(games, season, team1, date, 10, lower_bound)\n",
    "#             features_last10_team2 = get_last_N_games_average(games, season, team2, date, 10, lower_bound)\n",
    "\n",
    "#             features_last20_team1 = get_last_N_games_average(games, season, team1, date, 20, lower_bound)\n",
    "#             features_last20_team2 = get_last_N_games_average(games, season, team2, date, 20, lower_bound)\n",
    "            \n",
    "#             features_last30_team1 = get_last_N_games_average(games, season, team1, date, 30, lower_bound)\n",
    "#             features_last30_team2 = get_last_N_games_average(games, season, team2, date, 30, lower_bound)\n",
    "\n",
    "#             if features_last5_team1 is not None and features_last5_team2 is not None:\n",
    "                            \n",
    "#                 features_justlast5_diff.append(features_last5_team2 - features_last5_team1)\n",
    "            \n",
    "#                 vec2 = []\n",
    "#                 vec2.extend(features_last5_team2)\n",
    "#                 vec2.extend(features_last5_team1)\n",
    "#                 features_justlast5_vec.append(vec2)\n",
    "\n",
    "#                 vec3 = []\n",
    "#                 vec3.extend(mapping[season][team2][date][1] - mapping[season][team1][date][1])\n",
    "#                 vec3.extend(features_last5_team2 - features_last5_team1)\n",
    "#                 features_alsolast5_diff.append(vec3)\n",
    "\n",
    "#                 vec4 = []\n",
    "#                 vec4.extend(mapping[season][team2][date][1])\n",
    "#                 vec4.extend(mapping[season][team1][date][1])\n",
    "#                 vec4.extend(features_last5_team2)\n",
    "#                 vec4.extend(features_last5_team1)\n",
    "#                 features_alsolast5_vec.append(vec4)\n",
    "            \n",
    "#             if features_last3_team1 is not None and features_last3_team2 is not None:\n",
    "#                 vec4 = []\n",
    "#                 vec4.extend(mapping[season][team2][date][1])\n",
    "#                 vec4.extend(mapping[season][team1][date][1])\n",
    "#                 vec4.extend(features_last3_team2)\n",
    "#                 vec4.extend(features_last3_team1)\n",
    "#                 features_alsolast3_vec.append(vec4) \n",
    "\n",
    "#             if features_last10_team1 is not None and features_last10_team2 is not None:\n",
    "#                 vec4 = []\n",
    "#                 vec4.extend(mapping[season][team2][date][1])\n",
    "#                 vec4.extend(mapping[season][team1][date][1])\n",
    "#                 vec4.extend(features_last10_team2)\n",
    "#                 vec4.extend(features_last10_team1)\n",
    "#                 features_alsolast10_vec.append(vec4)\n",
    "\n",
    "#             if features_last20_team1 is not None and features_last20_team2 is not None:\n",
    "#                 vec4 = []\n",
    "#                 vec4.extend(mapping[season][team2][date][1])\n",
    "#                 vec4.extend(mapping[season][team1][date][1])\n",
    "#                 vec4.extend(features_last20_team2)\n",
    "#                 vec4.extend(features_last20_team1)\n",
    "#                 features_alsolast20_vec.append(vec4)\n",
    "            \n",
    "#             if features_last30_team1 is not None and features_last30_team2 is not None:\n",
    "#                 vec4 = []\n",
    "#                 vec4.extend(mapping[season][team2][date][1])\n",
    "#                 vec4.extend(mapping[season][team1][date][1])\n",
    "#                 vec4.extend(features_last30_team2)\n",
    "#                 vec4.extend(features_last30_team1)\n",
    "#                 features_alsolast30_vec.append(vec4)\n",
    "                \n",
    "            Narr.append([mapping[season][team2][date][0], mapping[season][team1][date][0]])\n",
    "            labelss.append(1 if score2 > score1 else 0)\n",
    "            labelsm.append(score2 - score1)\n",
    "            \n",
    "load_odds_file(\"nhl_odds_2012-13.csv\", 2012, mapping_simp, season_names[0], lower_bounds[0], labels_single_simp, labels_multi_simp, N_simp, simple_features)\n",
    "load_odds_file(\"nhl_odds_2013-14.csv\", 2013, mapping_simp, season_names[1], lower_bounds[1], labels_single_simp, labels_multi_simp, N_simp, simple_features)\n",
    "load_odds_file(\"nhl_odds_2014-15.csv\", 2014, mapping_simp, season_names[2], lower_bounds[2], labels_single_simp, labels_multi_simp, N_simp, simple_features)\n",
    "load_odds_file(\"nhl_odds_2015-16.csv\", 2015, mapping_simp, season_names[3], lower_bounds[3], labels_single_simp, labels_multi_simp, N_simp, simple_features)\n",
    "load_odds_file(\"nhl_odds_2016-17.csv\", 2016, mapping_simp, season_names[4], lower_bounds[4], labels_single_simp, labels_multi_simp, N_simp, simple_features)\n",
    "\n",
    "load_odds_file(\"nhl_odds_2012-13.csv\", 2012, mapping_adv, season_names[0], lower_bounds[0], labels_single_adv, labels_multi_adv, N_adv, advanced_features)\n",
    "load_odds_file(\"nhl_odds_2013-14.csv\", 2013, mapping_adv, season_names[1], lower_bounds[1], labels_single_adv, labels_multi_adv, N_adv, advanced_features)\n",
    "load_odds_file(\"nhl_odds_2014-15.csv\", 2014, mapping_adv, season_names[2], lower_bounds[2], labels_single_adv, labels_multi_adv, N_adv, advanced_features)\n",
    "load_odds_file(\"nhl_odds_2015-16.csv\", 2015, mapping_adv, season_names[3], lower_bounds[3], labels_single_adv, labels_multi_adv, N_adv, advanced_features)\n",
    "load_odds_file(\"nhl_odds_2016-17.csv\", 2016, mapping_adv, season_names[4], lower_bounds[4], labels_single_adv, labels_multi_adv, N_adv, advanced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import models from sklearn.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score as cv\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used when varying the hyperparameter \"N\".\n",
    "\n",
    "def get_mod_features_labels(cutoff, features, labels, Ns):\n",
    "    fts = []\n",
    "    lbls = []\n",
    "    for i in range(len(features)):\n",
    "        if Ns[i][0] < cutoff or Ns[i][1] < cutoff:\n",
    "            continue\n",
    "        fts.append(features[i])\n",
    "        lbls.append(labels[i])\n",
    "    return fts, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used previously for PCA. PCA didn't increase the accuracy\n",
    "# and removed our ability to understand the features intuitively\n",
    "# so we decided not to use it. Might be worth exploring in the future\n",
    "# if the curse of dimensionality is an issue.\n",
    "\n",
    "def PCA(clf, arr, labels):\n",
    "    best = (0, 0)\n",
    "    arr_std = StandardScaler().fit_transform(arr)\n",
    "    for i in range(1, len(arr[0])+1):\n",
    "        pca = decomposition.PCA(n_components = i)\n",
    "        arr_new = pca.fit_transform(arr_std)\n",
    "        score = cv(clf, arr_new, labels, cv=10)\n",
    "        avg = sum(score)/len(score)\n",
    "        print( i, avg )\n",
    "        if avg>best[0]:\n",
    "            best = (avg, i)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Used for debugging\n",
    "\n",
    "def get_playoff_feature(season, team, mapping, start):\n",
    "    while start not in mapping[season][team]:\n",
    "        start -= 1\n",
    "    return mapping[season][team][start][1]\n",
    "\n",
    "#print(get_playoff_feature(season_names[0], \"ANA\", mapping, seasons[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 0.592599277978 Test 0.578518722272\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - Simple Features\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, simple_features, labels_single_simp, N_simp)\n",
    "\n",
    "logreg = lm.LogisticRegression()\n",
    "logreg.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", logreg.score(fts, lbls), \"Test\", np.mean(cv(logreg, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 0.579241877256 Test 0.573831110473\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - Adv Features\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "\n",
    "logreg = lm.LogisticRegression()\n",
    "logreg.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", logreg.score(fts, lbls), \"Test\", np.mean(cv(logreg, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "LR N 0 Train 0.578880866426 Test 0.572024747147\n",
      "[[ 0.01458429 -0.00319405  0.01777834 -0.00214508  0.00222488  0.02248043\n",
      "  -0.1052488   0.12772923  0.08738838 -0.05980569 -0.03916719 -0.0060844\n",
      "  -0.03010745 -0.04210995  0.01588167  0.01511821 -0.03830144  0.04947413\n",
      "  -0.02174842  0.03038766 -0.11160686  0.09915301  0.08572148  0.01343153\n",
      "  -0.13889024 -0.05335984 -0.03630351  0.03938682 -0.07569033 -0.07192053\n",
      "  -0.00516776  0.01842421 -0.00248705  0.01563002  0.01808529 -0.01109703\n",
      "  -0.01051419 -0.2715411   0.298647   -0.19092727  0.16976221 -0.27068811]] [ 0.01484725]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - Adv - Feature Analysis\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "print(len(advanced_features[0]))\n",
    "logreg = lm.LogisticRegression()\n",
    "logreg.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", logreg.score(fts, lbls), \"Test\", np.mean(cv(logreg, fts, lbls, cv=10)))\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM C 0.01 Kernel rbf Train 0.545667870036 Test 0.54566796002\n",
      "SVM C 0.01 Kernel linear Train 0.586281588448 Test 0.577256485306\n",
      "SVM C 0.1 Kernel rbf Train 0.579963898917 Test 0.571489094542\n",
      "SVM C 0.1 Kernel linear Train 0.586101083032 Test 0.574908608088\n",
      "SVM C 0.5 Kernel rbf Train 0.597292418773 Test 0.576719530584\n",
      "SVM C 0.5 Kernel linear Train 0.587725631769 Test 0.57328373294\n",
      "SVM C 1 Kernel rbf Train 0.612635379061 Test 0.576359167871\n",
      "SVM C 1 Kernel linear Train 0.587003610108 Test 0.574367744665\n"
     ]
    }
   ],
   "source": [
    "# SVM - Simp Features\n",
    "\n",
    "Cs = [0.01, 0.1, 0.5, 1]\n",
    "kernels = [\"rbf\", \"linear\"]\n",
    "fts, lbls = get_mod_features_labels(0, simple_features, labels_single_simp, N_simp)\n",
    "\n",
    "for c in Cs:\n",
    "    for kernel in kernels:\n",
    "        svmo = svm.SVC(gamma=0.001, C=c, kernel=kernel)\n",
    "        svmo.fit(fts, lbls)\n",
    "        print (\"SVM\", \"C\", c, \"Kernel\", kernel, \"Train\", svmo.score(fts, lbls), \"Test\", np.mean(cv(svmo, fts, lbls, cv=10)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM C 0.01 Kernel rbf Train 0.545667870036 Test 0.54566796002\n",
      "SVM C 0.01 Kernel linear Train 0.563176895307 Test 0.560823603779\n",
      "SVM C 0.1 Kernel rbf Train 0.56155234657 Test 0.556497009867\n",
      "SVM C 0.1 Kernel linear Train 0.571119133574 Test 0.563351991117\n",
      "SVM C 0.5 Kernel rbf Train 0.570577617329 Test 0.563000109215\n",
      "SVM C 0.5 Kernel linear Train 0.571119133574 Test 0.565701827979\n",
      "SVM C 1 Kernel rbf Train 0.578700361011 Test 0.565343422558\n",
      "SVM C 1 Kernel linear Train 0.574368231047 Test 0.565520669742\n"
     ]
    }
   ],
   "source": [
    "# SVM - Adv Features\n",
    "\n",
    "Cs = [0.01, 0.1, 0.5, 1]\n",
    "kernels = [\"rbf\", \"linear\"]\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "\n",
    "for c in Cs:\n",
    "    for kernel in kernels:\n",
    "        svmo = svm.SVC(gamma=0.001, C=c, kernel=kernel)\n",
    "        svmo.fit(fts, lbls)\n",
    "        print (\"SVM\", \"C\", c, \"Kernel\", kernel, \"Train\", svmo.score(fts, lbls), \"Test\", np.mean(cv(svmo, fts, lbls, cv=10)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 1.0 Test 0.516614357354\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree - Simp\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, simple_features, labels_single_simp, N_simp)\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", tree.score(fts, lbls), \"Test\", np.mean(cv(tree, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 1.0 Test 0.511892530011\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree - Adv\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "\n",
    "logreg = DecisionTreeClassifier()\n",
    "tree.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", tree.score(fts, lbls), \"Test\", np.mean(cv(tree, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 0.98321299639 Test 0.522390864108\n"
     ]
    }
   ],
   "source": [
    "# Random Forest - Simp\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, simple_features, labels_single_simp, N_simp)\n",
    "\n",
    "tree = RandomForestClassifier()\n",
    "tree.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", tree.score(fts, lbls), \"Test\", np.mean(cv(tree, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR N 0 Train 0.985740072202 Test 0.510826751442\n"
     ]
    }
   ],
   "source": [
    "# Random Forest - Advanced\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "\n",
    "tree = RandomForestClassifier()\n",
    "tree.fit(fts, lbls)\n",
    "print (\"LR\", \"N\", 0, \"Train\", tree.score(fts, lbls), \"Test\", np.mean(cv(tree, fts, lbls, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # # for cutoff in range(0, 50, 10):\n",
    "\n",
    "# cutoff = 0\n",
    "\n",
    "# # NEUNET Size (5, 10) Solv lbfgs Act identity Alpha 1e-05 Train 0.578519855596 Test 0.578520997741\n",
    "# # NEUNET Size 5 Solv sgd Act relu Alpha 1e-05 Train 0.580866425993 Test 0.571109195381\n",
    "\n",
    "# Cs = [0.01, 0.1, 0.5, 1]\n",
    "# kernels = [\"rbf\", \"linear\"]\n",
    "\n",
    "# for c in Cs:\n",
    "#     for kernel in kernels:\n",
    "#         svmo = svm.SVC(gamma=0.001, C=c, kernel=kernel) #lm.LogisticRegression(), C=100.\n",
    "#         svmo.fit(fts, lbls)\n",
    "#         print (\"SVM\", \"C\", c, \"Kernel\", kernel, \"Train\", svmo.score(fts, lbls), \"Test\", np.mean(cv(svmo, fts, lbls, cv=10)))\n",
    "#         #     #print (\"Vec\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "\n",
    "# sizes = [(5), (10), (15), (5, 5), (10, 5), (5, 10), (8, 10, 12), (10, 15, 20)]\n",
    "# solvers = ['adam', 'lbfgs', 'sgd']\n",
    "# activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "# alphas = [1e-5, 1e-4, 1e-3]\n",
    "# for size in sizes:\n",
    "#     for solver in solvers:\n",
    "#         for activation in activations:\n",
    "#             for alpha in alphas:\n",
    "#                 clf = MLPClassifier(solver=solver, activation=activation, alpha=alpha, hidden_layer_sizes=size, random_state=1)\n",
    "#                 clf.fit(fts, lbls)\n",
    "                \n",
    "#                 print (\"NEUNET\", \"Size\", size, \"Solv\", solver, \"Act\", activation, \"Alpha\", alpha, \"Train\", clf.score(fts, lbls), \"Test\", np.mean(cv(clf, fts, lbls, cv=10)))\n",
    "\n",
    "# # # fts, lbls = get_mod_features_labels(cutoff, features_justlast5_diff, labels_single, N)\n",
    "# # # logreg = lm.LogisticRegression()\n",
    "# # # logreg.fit(fts, lbls)\n",
    "# # # print (\"Last5-D\", \"N\", cutoff, \"Training accuracy\", logreg.score(fts, lbls))\n",
    "\n",
    "# # # fts, lbls = get_mod_features_labels(cutoff, features_alsolast10_vec, labels_single, N)\n",
    "# # # logreg = svm.SVC(gamma=0.001, C=0.5) #lm.LogisticRegression()\n",
    "# # # logreg.fit(fts, lbls)\n",
    "# # # print (\"Last10\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "\n",
    "# # # # fts, lbls = get_mod_features_labels(cutoff, features_alsolast5_diff, labels_single, N)\n",
    "# # # # logreg = lm.LogisticRegression()\n",
    "# # # # logreg.fit(fts, lbls)\n",
    "# # # # print (\"All-D\", \"N\", cutoff, \"Training accuracy\", logreg.score(fts, lbls))\n",
    "\n",
    "# # # fts, lbls = get_mod_features_labels(cutoff, features_alsolast20_vec, labels_single, N)\n",
    "# # # logreg = svm.SVC(gamma=0.001, C=0.5) #lm.LogisticRegression()\n",
    "# # # logreg.fit(fts, lbls)\n",
    "# # # print (\"Last20\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "\n",
    "# # # fts, lbls = get_mod_features_labels(cutoff, features_alsolast30_vec, labels_single, N)\n",
    "# # # logreg = svm.SVC(gamma=0.001, C=0.5) #lm.LogisticRegression()\n",
    "# # # logreg.fit(fts, lbls)\n",
    "# # # print (\"Last30\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "\n",
    "# # # fts, lbls = get_mod_features_labels(cutoff, features_alsolast10_vec, labels_single, N)\n",
    "# # # logreg = svm.SVC(gamma=0.001, C=0.5) #lm.LogisticRegression()\n",
    "# # # logreg.fit(fts, lbls)\n",
    "# # # print (\"All-V-10\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUNET Size 5 Solv adam Act identity Alpha 1e-05 Train 0.536823104693 Test 0.528319509082\n",
      "NEUNET Size 5 Solv adam Act logistic Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size 5 Solv adam Act relu Alpha 1e-05 Train 0.508122743682 Test 0.524159085545\n",
      "NEUNET Size 5 Solv adam Act tanh Alpha 1e-05 Train 0.545848375451 Test 0.5458481402\n",
      "NEUNET Size 5 Solv lbfgs Act identity Alpha 1e-05 Train 0.577436823105 Test 0.570033650352\n",
      "NEUNET Size 5 Solv lbfgs Act logistic Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size 5 Solv lbfgs Act relu Alpha 1e-05 Train 0.589711191336 Test 0.577616839785\n",
      "NEUNET Size 5 Solv lbfgs Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size 15 Solv adam Act identity Alpha 1e-05 Train 0.549277978339 Test 0.529024282702\n",
      "NEUNET Size 15 Solv adam Act logistic Alpha 1e-05 Train 0.558122743682 Test 0.564618143842\n",
      "NEUNET Size 15 Solv adam Act relu Alpha 1e-05 Train 0.541155234657 Test 0.548906972856\n",
      "NEUNET Size 15 Solv adam Act tanh Alpha 1e-05 Train 0.546570397112 Test 0.543505462036\n",
      "NEUNET Size 15 Solv lbfgs Act identity Alpha 1e-05 Train 0.582310469314 Test 0.575800391823\n",
      "NEUNET Size 15 Solv lbfgs Act logistic Alpha 1e-05 Train 0.587184115523 Test 0.566778023478\n",
      "NEUNET Size 15 Solv lbfgs Act relu Alpha 1e-05 Train 0.579963898917 Test 0.572923364346\n",
      "NEUNET Size 15 Solv lbfgs Act tanh Alpha 1e-05 Train 0.597653429603 Test 0.577441551069\n",
      "NEUNET Size (5, 10) Solv adam Act identity Alpha 1e-05 Train 0.556317689531 Test 0.553431991865\n",
      "NEUNET Size (5, 10) Solv adam Act logistic Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size (5, 10) Solv adam Act relu Alpha 1e-05 Train 0.551083032491 Test 0.552713224907\n",
      "NEUNET Size (5, 10) Solv adam Act tanh Alpha 1e-05 Train 0.545848375451 Test 0.5458481402\n",
      "NEUNET Size (5, 10) Solv lbfgs Act identity Alpha 1e-05 Train 0.583393501805 Test 0.579416038531\n",
      "NEUNET Size (5, 10) Solv lbfgs Act logistic Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size (5, 10) Solv lbfgs Act relu Alpha 1e-05 Train 0.586101083032 Test 0.574194726712\n",
      "NEUNET Size (5, 10) Solv lbfgs Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n"
     ]
    }
   ],
   "source": [
    "# Simple NN \n",
    "fts, lbls = get_mod_features_labels(0, simple_features, labels_single_simp, N_simp)\n",
    "sizes = [(5), (15), (5, 10)] #  (4, 6, 8, 10, 12), (12, 10, 8, 6, 4), (4, 8, 12), (12, 8, 4), (14, 12, 10, 8), (8, 10, 12, 14)\n",
    "solvers = ['adam', 'lbfgs']\n",
    "activations = ['identity', 'logistic', 'relu', 'tanh']\n",
    "for size in sizes:\n",
    "    for solver in solvers:\n",
    "        for activation in activations:\n",
    "            clf = MLPClassifier(solver=solver, activation=activation, alpha=1e-5, hidden_layer_sizes=size, random_state=1)\n",
    "            clf.fit(fts, lbls)\n",
    "                \n",
    "            print (\"NEUNET\", \"Size\", size, \"Solv\", solver, \"Act\", activation, \"Alpha\", 1e-5, \"Train\", clf.score(fts, lbls), \"Test\", np.mean(cv(clf, fts, lbls, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUNET Size 5 Solv adam Act identity Alpha 1e-05 Train 0.529241877256 Test 0.552887532038\n",
      "NEUNET Size 5 Solv adam Act logistic Alpha 1e-05 Train 0.564801444043 Test 0.561366426846\n",
      "NEUNET Size 5 Solv adam Act relu Alpha 1e-05 Train 0.549458483755 Test 0.537343521246\n",
      "NEUNET Size 5 Solv adam Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.545306949189\n",
      "NEUNET Size 5 Solv lbfgs Act identity Alpha 1e-05 Train 0.58285198556 Test 0.575816668863\n",
      "NEUNET Size 5 Solv lbfgs Act logistic Alpha 1e-05 Train 0.580866425993 Test 0.559926288697\n",
      "NEUNET Size 5 Solv lbfgs Act relu Alpha 1e-05 Train 0.580324909747 Test 0.572026056321\n",
      "NEUNET Size 5 Solv lbfgs Act tanh Alpha 1e-05 Train 0.545848375451 Test 0.545125790952\n",
      "NEUNET Size 15 Solv adam Act identity Alpha 1e-05 Train 0.554693140794 Test 0.536459889558\n",
      "NEUNET Size 15 Solv adam Act logistic Alpha 1e-05 Train 0.566606498195 Test 0.56859578179\n",
      "NEUNET Size 15 Solv adam Act relu Alpha 1e-05 Train 0.531588447653 Test 0.554306494625\n",
      "NEUNET Size 15 Solv adam Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.545123833661\n",
      "NEUNET Size 15 Solv lbfgs Act identity Alpha 1e-05 Train 0.576895306859 Test 0.575998145278\n",
      "NEUNET Size 15 Solv lbfgs Act logistic Alpha 1e-05 Train 0.576534296029 Test 0.559738615173\n",
      "NEUNET Size 15 Solv lbfgs Act relu Alpha 1e-05 Train 0.573826714801 Test 0.562265701572\n",
      "NEUNET Size 15 Solv lbfgs Act tanh Alpha 1e-05 Train 0.546570397112 Test 0.554869516372\n",
      "NEUNET Size (5, 10) Solv adam Act identity Alpha 1e-05 Train 0.555415162455 Test 0.546028974379\n",
      "NEUNET Size (5, 10) Solv adam Act logistic Alpha 1e-05 Train 0.569855595668 Test 0.564248651025\n",
      "NEUNET Size (5, 10) Solv adam Act relu Alpha 1e-05 Train 0.553429602888 Test 0.551621057607\n",
      "NEUNET Size (5, 10) Solv adam Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.545305971132\n",
      "NEUNET Size (5, 10) Solv lbfgs Act identity Alpha 1e-05 Train 0.578880866426 Test 0.575282000196\n",
      "NEUNET Size (5, 10) Solv lbfgs Act logistic Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size (5, 10) Solv lbfgs Act relu Alpha 1e-05 Train 0.545667870036 Test 0.54566796002\n",
      "NEUNET Size (5, 10) Solv lbfgs Act tanh Alpha 1e-05 Train 0.545667870036 Test 0.545306949189\n"
     ]
    }
   ],
   "source": [
    "# Advanced NN \n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "sizes = [(5), (15), (5, 10)] #  (4, 6, 8, 10, 12), (12, 10, 8, 6, 4), (4, 8, 12), (12, 8, 4), (14, 12, 10, 8), (8, 10, 12, 14)\n",
    "solvers = ['adam', 'lbfgs']\n",
    "activations = ['identity', 'logistic', 'relu', 'tanh']\n",
    "for size in sizes:\n",
    "    for solver in solvers:\n",
    "        for activation in activations:\n",
    "            clf = MLPClassifier(solver=solver, activation=activation, alpha=1e-5, hidden_layer_sizes=size, random_state=1)\n",
    "            clf.fit(fts, lbls)\n",
    "                \n",
    "            print (\"NEUNET\", \"Size\", size, \"Solv\", solver, \"Act\", activation, \"Alpha\", 1e-5, \"Train\", clf.score(fts, lbls), \"Test\", np.mean(cv(clf, fts, lbls, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUNET Size 5 Solv lbfgs Act relu Alpha 1e-05 Train 0.580324909747 Test 0.572026056321\n",
      "5540 5540\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis - Use this Model\n",
    "\n",
    "fts, lbls = get_mod_features_labels(0, advanced_features, labels_single_adv, N_adv)\n",
    "size = (5)\n",
    "solver = 'lbfgs'\n",
    "activation = 'relu'\n",
    "\n",
    "clf = MLPClassifier(solver=solver, activation=activation, alpha=1e-5, hidden_layer_sizes=size, random_state=1)\n",
    "clf.fit(fts, lbls)\n",
    "print (\"NEUNET\", \"Size\", size, \"Solv\", solver, \"Act\", activation, \"Alpha\", 1e-5, \"Train\", clf.score(fts, lbls), \"Test\", np.mean(cv(clf, fts, lbls, cv=10)))\n",
    "\n",
    "print(len(fts), len(lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554 554\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis - Take dev set\n",
    "\n",
    "indeces = list(range(len(lbls)))\n",
    "np.random.shuffle(indeces)\n",
    "indeces = indeces[0:int(len(indeces)/10)]\n",
    "\n",
    "fts2 = []\n",
    "lbls2 = []\n",
    "for index in indeces:\n",
    "    fts2.append(fts[index])\n",
    "    lbls2.append(lbls[index])\n",
    "\n",
    "fts = fts2\n",
    "lbls = lbls2\n",
    "\n",
    "print(len(fts), len(lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Win:  0.5588235294117647 1088\n",
      "Both Loss:  0.5627871362940275 1306\n",
      "1 Win/1 Loss:  0.5950413223140496 3146\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "\n",
    "both_win = {'correct': 0, 'total': 0}\n",
    "both_loss = {'correct': 0, 'total': 0}\n",
    "either = {'correct': 0, 'total': 0}\n",
    "\n",
    "for i in range(len(lbls)):\n",
    "    team1 = (fts[i][int(len(fts[i])/2) - 1])\n",
    "    team2 = (fts[i][len(fts[i]) - 1])\n",
    "    label = lbls[i]\n",
    "    predict = clf.predict([fts[i]])\n",
    "    if team1 > 0.5 and team2 > 0.5:\n",
    "        both_win['total'] += 1\n",
    "        both_win['correct'] += (1 if predict[0] == label else 0)\n",
    "    elif team1 < 0.5 and team2 < 0.5:\n",
    "        both_loss['total'] += 1\n",
    "        both_loss['correct'] += (1 if predict[0] == label else 0)\n",
    "    else: \n",
    "        either['total'] += 1\n",
    "        either['correct'] += (1 if predict[0] == label else 0)\n",
    "    #print(team1, team2, label, predict)\n",
    "\n",
    "print(\"Both Win: \", both_win['correct']*1./both_win['total'], both_win['total'])\n",
    "print(\"Both Loss: \", both_loss['correct']*1./both_loss['total'], both_loss['total'])\n",
    "print(\"1 Win/1 Loss: \", either['correct']*1./either['total'], either['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Win:  0.5238970588235294 1088\n",
      "Both Loss:  0.5474732006125574 1306\n",
      "1 Win/1 Loss:  0.5718372536554355 3146\n"
     ]
    }
   ],
   "source": [
    "# Feature Analysis\n",
    "\n",
    "both_win = {'better_wins': 0, 'total': 0}\n",
    "both_loss = {'better_wins': 0, 'total': 0}\n",
    "either = {'better_wins': 0, 'total': 0}\n",
    "\n",
    "for i in range(len(lbls)):\n",
    "    team1 = (fts[i][int(len(fts[i])/2) - 1])\n",
    "    team2 = (fts[i][len(fts[i]) - 1])\n",
    "    label = lbls[i]\n",
    "    predict = clf.predict([fts[i]])\n",
    "    pred = 0 if team2 > team1 else 1\n",
    "    if team1 > 0.5 and team2 > 0.5:\n",
    "        both_win['total'] += 1\n",
    "        both_win['better_wins'] += (1 if pred == label else 0)\n",
    "    elif team1 < 0.5 and team2 < 0.5:\n",
    "        both_loss['total'] += 1\n",
    "        both_loss['better_wins'] += (1 if pred == label else 0)\n",
    "    else: \n",
    "        either['total'] += 1\n",
    "        either['better_wins'] += (1 if pred == label else 0)\n",
    "    #print(team1, team2, label, predict)\n",
    "\n",
    "print(\"Both Win: \", both_win['better_wins']*1./both_win['total'], both_win['total'])\n",
    "print(\"Both Loss: \", both_loss['better_wins']*1./both_loss['total'], both_loss['total'])\n",
    "print(\"1 Win/1 Loss: \", either['better_wins']*1./either['total'], either['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49 - 0.51   451   0.5144124168514412\n",
      "0.48 - 0.52   462   0.5281385281385281\n",
      "0.47 - 0.53   410   0.5585365853658537\n",
      "0.46 - 0.54   457   0.5470459518599562\n",
      "0.45 - 0.55   461   0.5574837310195228\n",
      "0.425 - 0.575   978   0.5501022494887525\n",
      "0.4 - 0.6   747   0.5689424364123159\n",
      "0.3 - 0.7   1397   0.654974946313529\n",
      "0.0 - 1.0   177   0.7062146892655368\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis: 2\n",
    "\n",
    "distances = [.01, .02, .03, .04, .05, .075, .1, .2, .5]\n",
    "corrects = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "totals = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(len(lbls)):\n",
    "    predict = clf.predict_proba([fts[i]])[0]\n",
    "    predict = predict[0] if predict[0] > predict[1] else predict[1]\n",
    "    label = lbls[i]\n",
    "    for j in range(len(distances)):\n",
    "        if predict - 0.5 <= distances[j]:\n",
    "            totals[j] += 1\n",
    "            if clf.predict([fts[i]]) == label:\n",
    "                corrects[j] += 1\n",
    "            break\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    print((0.5-distances[i]), \"-\", (0.5+distances[i]), \" \", totals[i], \" \", corrects[i]*1./totals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dev set 0.580324909747\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on dev set\", clf.score(fts, lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N: 0, 10, 20, 30, 40, 50\n",
    "# models: logreg, svm (linear), svm (default - rbf), neural net\n",
    "\n",
    "# Playoff results, directly imported (messy but works!)\n",
    "\n",
    "playoff_results = {\n",
    "    \"12-13\":[\n",
    "        [\"PIT\", \"NYI\", 4, 2, ],\n",
    "        [\"MTL\", \"OTT\", 1, 4],\n",
    "        [\"WSH\", \"NYR\", 3, 4],\n",
    "        [\"BOS\", \"TOR\", 4, 3],\n",
    "        [\"CHI\", \"MIN\", 4, 1],\n",
    "        [\"ANA\", \"DET\", 3, 4],\n",
    "        [\"VAN\", \"S.J\", 0, 4],\n",
    "        [\"STL\", \"L.A\", 2, 4],\n",
    "        [\"PIT\", \"OTT\", 4, 1],\n",
    "        [\"BOS\", \"NYR\", 4, 1],\n",
    "        [\"CHI\", \"DET\", 4, 3],\n",
    "        [\"L.A\", \"S.J\", 4, 3],\n",
    "        [\"PIT\", \"BOS\", 0, 4],\n",
    "        [\"CHI\", \"L.A\", 4, 1],\n",
    "        [\"BOS\", \"CHI\", 2, 4]\n",
    "    ],\n",
    "    \"13-14\":[\n",
    "        [\"BOS\", \"DET\", 4, 1],\n",
    "        [\"T.B\", \"MTL\", 0, 4],\n",
    "        [\"PIT\", \"CBJ\", 4, 2],\n",
    "        [\"NYR\", \"PHI\", 4, 3],\n",
    "        [\"COL\", \"MIN\", 3, 4],\n",
    "        [\"STL\", \"CHI\", 2, 4],\n",
    "        [\"ANA\", \"DAL\", 4, 2],\n",
    "        [\"S.J\", \"L.A\", 3, 4],\n",
    "        [\"BOS\", \"MTL\", 3, 4],\n",
    "        [\"PIT\", \"NYR\", 3, 4],\n",
    "        [\"MIN\", \"CHI\", 2, 4],\n",
    "        [\"ANA\", \"L.A\", 3, 4],\n",
    "        [\"MTL\", \"NYR\", 2, 4],\n",
    "        [\"CHI\", \"L.A\", 3, 4],\n",
    "        [\"NYR\", \"L.A\", 1, 4]\n",
    "    ],\n",
    "    \"14-15\":[\n",
    "        [\"MTL\", \"OTT\", 4, 2],\n",
    "        [\"T.B\", \"DET\", 4, 3],\n",
    "        [\"NYR\", \"PIT\", 4, 1],\n",
    "        [\"WSH\", \"NYI\", 4, 3],\n",
    "        [\"STL\", \"MIN\", 2, 4],\n",
    "        [\"NSH\", \"CHI\", 2, 4],\n",
    "        [\"ANA\", \"WPG\", 4, 0],\n",
    "        [\"VAN\", \"CGY\", 2, 4],\n",
    "        [\"MTL\", \"T.B\", 2, 4],\n",
    "        [\"NYR\", \"WSH\", 4, 3],\n",
    "        [\"MIN\", \"CHI\", 0, 4],\n",
    "        [\"ANA\", \"CGY\", 4, 1],\n",
    "        [\"T.B\", \"NYR\", 4, 3],\n",
    "        [\"CHI\", \"ANA\", 4, 3],\n",
    "        [\"T.B\", \"CHI\", 2, 4]\n",
    "    ],\n",
    "    \"15-16\":[\n",
    "        [\"FLA\", \"NYI\", 2, 4],\n",
    "        [\"T.B\", \"DET\", 4, 1],\n",
    "        [\"WSH\", \"PHI\", 4, 2],\n",
    "        [\"PIT\", \"NYR\", 4, 1],\n",
    "        [\"DAL\", \"MIN\", 4, 2],\n",
    "        [\"STL\", \"CHI\", 4, 3],\n",
    "        [\"ANA\", \"NSH\", 3, 4],\n",
    "        [\"L.A\", \"S.J\", 1, 4],\n",
    "        [\"NYI\", \"T.B\", 1, 4],\n",
    "        [\"WSH\", \"PIT\", 2, 4],\n",
    "        [\"DAL\", \"STL\", 3, 4],\n",
    "        [\"NSH\", \"S.J\", 3, 4],\n",
    "        [\"T.B\", \"PIT\", 3, 4],\n",
    "        [\"STL\", \"S.J\", 2, 4],\n",
    "        [\"PIT\", \"S.J\", 4, 2]\n",
    "    ],\n",
    "    \"16-17\":[\n",
    "        [\"MTL\", \"NYR\", 2, 4],\n",
    "        [\"OTT\", \"BOS\", 4, 2],\n",
    "        [\"WSH\", \"TOR\", 4, 2],\n",
    "        [\"PIT\", \"CBJ\", 4, 1],\n",
    "        [\"CHI\", \"NSH\", 0, 4],\n",
    "        [\"MIN\", \"STL\", 1, 4],\n",
    "        [\"ANA\", \"CGY\", 4, 0],\n",
    "        [\"EDM\", \"S.J\", 4, 2],\n",
    "        [\"NYR\", \"OTT\", 2, 4],\n",
    "        [\"WSH\", \"PIT\", 3, 4],\n",
    "        [\"NSH\", \"STL\", 4, 2],\n",
    "        [\"ANA\", \"EDM\", 4, 3],\n",
    "        [\"OTT\", \"PIT\", 3, 4],\n",
    "        [\"NSH\", \"ANA\", 4, 2],\n",
    "        [\"PIT\", \"NSH\", 4, 2]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playoff_features = []\n",
    "playoff_labels = []\n",
    "\n",
    "for date in playoff_results:\n",
    "    for game in playoff_results[date]:\n",
    "        \n",
    "        start = seasons[season_names.index(date)]\n",
    "        team1 = get_playoff_feature(date, game[0], mapping, start)\n",
    "        team2 = get_playoff_feature(date, game[1], mapping, start)\n",
    "        vec = []\n",
    "        vec.extend(team1)\n",
    "        vec.extend(team2)\n",
    "        playoff_features.append(vec)\n",
    "        playoff_labels.append(1 if game[3] > game[2] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logregp = lm.LogisticRegression()\n",
    "logregp.fit(playoff_features, playoff_labels)\n",
    "print (\"LR\", \"N\", cutoff, \"Train\", logregp.score(playoff_features, playoff_labels), \"Test\", np.mean(cv(logregp, playoff_features, playoff_labels, cv=10)))\n",
    "\n",
    "# SVM C 100 Kernel rbf Train 1.0 Test 0.7 -> can get 70% accuracy on predicting playoffs\n",
    "Cs = [0.01, 0.1, 0.5, 1, 10, 20, 40, 50, 75, 100, 1000, 10000]\n",
    "kernels = [\"rbf\", \"linear\", \"poly\", \"sigmoid\"]\n",
    "\n",
    "fts2 = playoff_features\n",
    "lbls2 = playoff_labels\n",
    "for c in Cs:\n",
    "    for kernel in kernels:\n",
    "        svmo = svm.SVC(gamma=0.001, C=c, kernel=kernel) #lm.LogisticRegression(), C=100.\n",
    "        svmo.fit(fts2, lbls2)\n",
    "        print (\"SVM\", \"C\", c, \"Kernel\", kernel, \"Train\", svmo.score(fts2, lbls2), \"Test\", np.mean(cv(svmo, fts2, lbls2, cv=10)))\n",
    "        #     #print (\"Vec\", \"N\", cutoff, \"Training accuracy\", np.mean(cv(logreg, fts, lbls, cv=10)), logreg.score(fts, lbls))\n",
    "    \n",
    "sizes = [(5), (10), (15), (5, 5), (10, 5), (5, 10), (8, 10, 12), (10, 15, 20)]\n",
    "solvers = ['adam', 'lbfgs', 'sgd']\n",
    "activations = ['identity', 'logistic', 'tanh', 'relu']\n",
    "alphas = [1e-5, 1e-4, 1e-3]\n",
    "for size in sizes:\n",
    "    for solver in solvers:\n",
    "        for activation in activations:\n",
    "            for alpha in alphas:\n",
    "                clf = MLPClassifier(solver=solver, activation=activation, alpha=alpha, hidden_layer_sizes=size, random_state=1)\n",
    "                clf.fit(fts2, lbls2)\n",
    "                \n",
    "                print (\"NEUNET\", \"Size\", size, \"Solv\", solver, \"Act\", activation, \"Alpha\", alpha, \"Train\", clf.score(fts2, lbls2), \"Test\", np.mean(cv(clf, fts2, lbls2, cv=10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from math import factorial\n",
    "\n",
    "def comb(n, k):\n",
    "    return factorial(n) / factorial(k) / factorial(n - k)\n",
    "\n",
    "# PROB THAT A wins ONE GAME\n",
    "def predictSeries(prob):\n",
    "    five = comb(4, 3)\n",
    "    six = comb(5, 3)\n",
    "    seven= comb(6, 3)\n",
    "    A_4 = prob**4\n",
    "    B_4 = (1-prob)**4\n",
    "    A_5 = five*(prob**4)*(1-prob)\n",
    "    B_5 = five*((1-prob)**4)*(prob)\n",
    "    A_6 = six*(prob**4)*((1-prob)**2)\n",
    "    B_6 = six*((1-prob)**4)*(prob**2)\n",
    "    A_7 = seven*(prob**4)*((1-prob)**3)\n",
    "    B_7 = seven*((1-prob)**4)*(prob**3)\n",
    "    return [A_4, B_4, A_5, B_5, A_6, B_6, A_7, B_7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = 0\n",
    "fts, lbls = get_mod_features_labels(cutoff, features_vec, labels_single, N)\n",
    "logreg = lm.LogisticRegression()\n",
    "logreg.fit(fts, lbls)\n",
    "\n",
    "print(\"Accuracy on regular season\", np.mean(cv(logreg, fts, lbls, cv=10)))\n",
    "\n",
    "cor = 0\n",
    "tot = 0\n",
    "for i in range(len(playoff_features)):\n",
    "    ft = playoff_features[i]\n",
    "    lbl = playoff_labels[i]\n",
    "    prob = logreg.predict_proba([ft])[0][0] # returns prob for 0, 1 -> probability that team2 wins one game\n",
    "    series = predictSeries(prob)\n",
    "    sum_prob = series[0] + series[2] + series[4] + series[6] # probability that team2 wins series\n",
    "    pred_label = 1 if sum_prob > 0.5 else 0\n",
    "    if lbl == pred_label:\n",
    "        cor += 1\n",
    "    tot += 1\n",
    "print(\"Accuracy on playoff season\", (cor*1./tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_cutoffs = [20130430, 20140416, 20150415, 20160413, 20170412]\n",
    "\n",
    "probs = {} # Season -> Date -> []\n",
    "payoffs = {} # Season -> Date -> {} 'H', 'V'\n",
    "for name in season_names:\n",
    "    probs[name] = {}\n",
    "    payoffs[name] = {}\n",
    "\n",
    "def load_playoffs_file(model, txtfile, year, mapping, season, date_cutoff):\n",
    "    with open(txtfile, 'r') as f:\n",
    "        lines = [row for row in csv.reader(f.read().splitlines())]\n",
    "        lines.pop(0)\n",
    "        \n",
    "        for i in range(0, len(lines), 2):\n",
    "            line1 = lines[i]\n",
    "            line2 = lines[i+1]\n",
    "            \n",
    "            if line1[7].strip() == '' or line2[7].strip() == '':\n",
    "                continue\n",
    "            \n",
    "            date = convert_odds_date(line1[0], year)\n",
    "            team1 = name_cities_mapping[line1[3].strip()] # Visitor\n",
    "            team2 = name_cities_mapping[line2[3].strip()] # Home\n",
    "            score1 = int(line1[7])\n",
    "            score2 = int(line2[7])\n",
    "            \n",
    "            #print(date, date_cutoff)\n",
    "            \n",
    "            if date < date_cutoff:\n",
    "                continue\n",
    "                        \n",
    "            # feature vector 1,2\n",
    "            start = date\n",
    "            feature_team1 = get_playoff_feature(season, team1, mapping, start)\n",
    "            feature_team2 = get_playoff_feature(season, team2, mapping, start)\n",
    "            vec = []\n",
    "            vec.extend(feature_team1)\n",
    "            vec.extend(feature_team2)\n",
    "            vec = [vec]\n",
    "            prob2 = model.predict_proba(vec)[0][0] # for team2\n",
    "            #series = predictSeries(prob)\n",
    "            \n",
    "            #print(line1)\n",
    "            \n",
    "            odds1 = line1[10]\n",
    "            odds1 = int(odds1[odds1.find(\"(\")+1:odds1.find(\")\")])\n",
    "            odds1 = (100-odds1)/(-odds1) if odds1 < 0 else (100+odds1)/(100)\n",
    "            odds2 = line2[10]\n",
    "            odds2 = int(odds2[odds2.find(\"(\")+1:odds2.find(\")\")])\n",
    "            odds2 = (100-odds2)/(-odds2) if odds2 < 0 else (100+odds2)/(100)\n",
    "            \n",
    "            if date not in probs[season]:\n",
    "                probs[season][date] = []\n",
    "                payoffs[season][date] = []\n",
    "            \n",
    "            probs[season][date].append(prob2)\n",
    "            mmap = {}\n",
    "            mmap['H'] = odds2\n",
    "            mmap['A'] = odds1\n",
    "            payoffs[season][date].append(mmap)\n",
    "            \n",
    "#load_playoffs_file(logreg, \"nhl_odds_2012-13.csv\", 2012, mapping, season_names[0], date_cutoffs[0])\n",
    "#load_playoffs_file(logreg, \"nhl_odds_2013-14.csv\", 2013, mapping, season_names[1], date_cutoffs[1])\n",
    "load_playoffs_file(logreg, \"nhl_odds_2014-15.csv\", 2014, mapping, season_names[2], date_cutoffs[2])\n",
    "load_playoffs_file(logreg, \"nhl_odds_2015-16.csv\", 2015, mapping, season_names[3], date_cutoffs[3])\n",
    "load_playoffs_file(logreg, \"nhl_odds_2016-17.csv\", 2016, mapping, season_names[4], date_cutoffs[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(probs[season_names[2]][20150415])\n",
    "print(payoffs[season_names[2]][20150415])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('probs.pickle', 'wb') as handle:\n",
    "    pickle.dump(probs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('payoffs.pickle', 'wb') as handle:\n",
    "    pickle.dump(payoffs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = 0\n",
    "fts, lbls = get_mod_features_labels(cutoff, features_vec, labels_single, N)\n",
    "logreg = lm.LogisticRegression()\n",
    "logreg.fit(fts, lbls)\n",
    "confusion_matrix = metrics.confusion_matrix(playoff_labels, logreg.predict(playoff_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = [\"Visitor Wins\", \"Home Wins\"]\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix, classes=class_names,\n",
    "                      title='Confusion matrix on test set')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(confusion_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
